{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-20T18:17:43.596308Z",
     "start_time": "2025-07-20T18:17:43.593584Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from spacy import attrs\n",
    "from spacy.symbols import VERB, NOUN, ADV, ADJ"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Important constants",
   "id": "412fd65b46889ebb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:17:43.610267Z",
     "start_time": "2025-07-20T18:17:43.608805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TEXT_COLUMN = 'text'\n",
    "Y_COLUMN = 'author'"
   ],
   "id": "16fd62a7d69877a5",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Implementation of Stratified K Folds",
   "id": "7091b7f722febe88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:17:43.626966Z",
     "start_time": "2025-07-20T18:17:43.624640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_pipeline(df, nlp_pipeline, pipeline_name = ''):\n",
    "    y = df[Y_COLUMN].copy()\n",
    "    x = df[TEXT_COLUMN].copy()\n",
    "    #use stratified splits to solve the unbalance in author classes\n",
    "    rskf = StratifiedKFold(n_splits = 5, random_state = 1, shuffle = True)\n",
    "    losses = []\n",
    "\n",
    "    #getting train data and test data for the cross validation\n",
    "    for train_index, test_index in rskf.split(x,y):\n",
    "        x_train , x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        #training the nlp on the training data\n",
    "        nlp_pipeline.fit(x_train, y_train)\n",
    "        #add the losses and compares how confident the model is in predicting the author ( lower loss means more confidence, higher less confidence)\n",
    "        losses.append(metrics.log_loss(y_test, nlp_pipeline.predict_proba(x_test)))\n",
    "\n",
    "    print(f'{pipeline_name} kfolds log losses: {str([str(round(x,3)) for x in sorted(losses)])}')\n",
    "    print(f'{pipeline_name} mean log loss: {round(np.mean(losses),3)}')"
   ],
   "id": "ba76e4e42137eb82",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:17:43.660075Z",
     "start_time": "2025-07-20T18:17:43.633048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#id column not helpful so we are ignoring it from the train data\n",
    "train_df = pd.read_csv('Data/train/train.csv', usecols=[TEXT_COLUMN, Y_COLUMN])"
   ],
   "id": "7de103ae48ec9165",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:17:44.596135Z",
     "start_time": "2025-07-20T18:17:43.667878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unigram_pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),#takes the text and then tokenize it and converts it into a vector of word frequency\n",
    "    ('mnb', MultinomialNB()),#applies naive bayes class , trains model,\n",
    "                        ])\n",
    "test_pipeline(train_df,unigram_pipe,pipeline_name='Unigrams only')"
   ],
   "id": "7d7619d3d7b2b559",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams only kfolds log losses: ['0.448', '0.452', '0.458', '0.476', '0.486']\n",
      "Unigrams only mean log loss: 0.464\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:17:44.602369Z",
     "start_time": "2025-07-20T18:17:44.599471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UnigramPredictions(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.unigram_mnb = Pipeline([\n",
    "            ('text', CountVectorizer()),\n",
    "            ('mnb', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "    #train the naive bayes model\n",
    "    def fit(self, x, y = None):\n",
    "        self.unigram_mnb.fit(x,y)\n",
    "        return self\n",
    "\n",
    "    def add_unigram_predictions(self,text_series):\n",
    "        #reseting dataframe index so they are equal with row number for later merges\n",
    "        df = pd.DataFrame(text_series.reset_index(drop=True))\n",
    "\n",
    "        #making the unigram prob and label them with the prediction class(the author in this case)\n",
    "        unigram_predictions = pd.DataFrame(\n",
    "            self.unigram_mnb.predict_proba(text_series),\n",
    "            columns = ['naive_bayes_pred_' + x for x in self.unigram_mnb.classes_]#class is name of author\n",
    "                    )\n",
    "        #remove one column because the last column is one minus the sum of the other two\n",
    "        del unigram_predictions[unigram_predictions.columns[0]]\n",
    "        df = df.merge(unigram_predictions, left_index = True, right_index = True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, text_series):\n",
    "        return self.add_unigram_predictions(text_series)\n"
   ],
   "id": "1ff69499ad233",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:17:44.794900Z",
     "start_time": "2025-07-20T18:17:44.619060Z"
    }
   },
   "cell_type": "code",
   "source": "NLP = spacy.load('en_core_web_sm', disable=['parser','ner'])",
   "id": "e547ab3812168200",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:17:44.805144Z",
     "start_time": "2025-07-20T18:17:44.802366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PartOfSpeechFeatures(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.NLP = NLP\n",
    "        #Store number of cpus available when we use multithreading\n",
    "        self.num_cores = cpu_count()\n",
    "\n",
    "    #count the number of nouns,verbs,adjectives\n",
    "    def part_of_speechiness(self, pos_counts, part_of_speech):\n",
    "        if eval(part_of_speech) in pos_counts:#we are using eval to see if the part_of_speech is valid\n",
    "            return pos_counts[eval(part_of_speech).numerator]\n",
    "        return 0\n",
    "\n",
    "    def add_pos_features(self, df):\n",
    "        text_series = df[TEXT_COLUMN]\n",
    "\n",
    "        df['doc'] = [i for i in self.NLP(text_series.values, n_threads = self.num_cores)]\n",
    "        df['pos_counts'] = df['doc'].apply(lambda x: x.count_by(attrs.POS))\n",
    "\n",
    "        df['sentence_length'] = df['doc'].str.len()\n",
    "\n",
    "        for part_of_speech in ('NOUN', 'VERB', 'ADJ', 'ADV'):\n",
    "            df[f'{part_of_speech.lower()}iness'] = df['pos_counts'].apply(\n",
    "                lambda x: self.part_of_speechiness(x, part_of_speech))\n",
    "            df[f'{part_of_speech.lower()}iness'] /= df['sentence_length']\n",
    "        df['avg_word_length'] = df['doc'].apply(\n",
    "            lambda x: sum([len(word) for word in x])) / df['sentence_length']\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fit(self,df):\n",
    "        return self.add_pos_features(df.copy())\n"
   ],
   "id": "8257f79fcbc713a6",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:17:44.812981Z",
     "start_time": "2025-07-20T18:17:44.811026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DropStringColumns(TransformerMixin):\n",
    "    def fit(self, x, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        for col, dtype in zip(df.columns, df.dtypes):\n",
    "            if dtype == 'object':\n",
    "                del df[col]\n",
    "        return df"
   ],
   "id": "2362ffcb99670f1e",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Now we are going to train the model",
   "id": "e2b80ff05c766eb9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:17:44.865579Z",
     "start_time": "2025-07-20T18:17:44.818020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logit_all_features_pipe = Pipeline([\n",
    "    ('uni', UnigramPredictions()),\n",
    "    ('nlp', PartOfSpeechFeatures()),\n",
    "    ('clean', DropStringColumns()),\n",
    "    ('clf', LogisticRegression())\n",
    "                               ])\n",
    "\n",
    "test_pipeline(train_df, logit_all_features_pipe)"
   ],
   "id": "e57742dc1c348d00",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<__main__.PartOfSpeechFeatures object at 0x000001A499B0C1A0>' (type <class '__main__.PartOfSpeechFeatures'>) doesn't",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[43]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      1\u001B[39m logit_all_features_pipe = Pipeline([\n\u001B[32m      2\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33muni\u001B[39m\u001B[33m'\u001B[39m, UnigramPredictions()),\n\u001B[32m      3\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mnlp\u001B[39m\u001B[33m'\u001B[39m, PartOfSpeechFeatures()),\n\u001B[32m      4\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mclean\u001B[39m\u001B[33m'\u001B[39m, DropStringColumns()),\n\u001B[32m      5\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mclf\u001B[39m\u001B[33m'\u001B[39m, LogisticRegression())\n\u001B[32m      6\u001B[39m                                ])\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[43mtest_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_all_features_pipe\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[36]\u001B[39m\u001B[32m, line 13\u001B[39m, in \u001B[36mtest_pipeline\u001B[39m\u001B[34m(df, nlp_pipeline, pipeline_name)\u001B[39m\n\u001B[32m     11\u001B[39m y_train, y_test = y[train_index], y[test_index]\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m#training the nlp on the training data\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m \u001B[43mnlp_pipeline\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m#add the losses and compares how confident the model is in predicting the author ( lower loss means more confidence, higher less confidence)\u001B[39;00m\n\u001B[32m     15\u001B[39m losses.append(metrics.log_loss(y_test, nlp_pipeline.predict_proba(x_test)))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1358\u001B[39m     estimator._validate_params()\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1361\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1362\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1363\u001B[39m     )\n\u001B[32m   1364\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:655\u001B[39m, in \u001B[36mPipeline.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m    648\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    649\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mThe `transform_input` parameter can only be set if metadata \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    650\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mrouting is enabled. You can enable metadata routing using \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    651\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    652\u001B[39m     )\n\u001B[32m    654\u001B[39m routed_params = \u001B[38;5;28mself\u001B[39m._check_method_params(method=\u001B[33m\"\u001B[39m\u001B[33mfit\u001B[39m\u001B[33m\"\u001B[39m, props=params)\n\u001B[32m--> \u001B[39m\u001B[32m655\u001B[39m Xt = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraw_params\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    656\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(\u001B[33m\"\u001B[39m\u001B[33mPipeline\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m._log_message(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m.steps) - \u001B[32m1\u001B[39m)):\n\u001B[32m    657\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._final_estimator != \u001B[33m\"\u001B[39m\u001B[33mpassthrough\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:563\u001B[39m, in \u001B[36mPipeline._fit\u001B[39m\u001B[34m(self, X, y, routed_params, raw_params)\u001B[39m\n\u001B[32m    561\u001B[39m \u001B[38;5;66;03m# shallow copy of steps - this should really be steps_\u001B[39;00m\n\u001B[32m    562\u001B[39m \u001B[38;5;28mself\u001B[39m.steps = \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m.steps)\n\u001B[32m--> \u001B[39m\u001B[32m563\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_validate_steps\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    564\u001B[39m \u001B[38;5;66;03m# Setup the memory\u001B[39;00m\n\u001B[32m    565\u001B[39m memory = check_memory(\u001B[38;5;28mself\u001B[39m.memory)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:340\u001B[39m, in \u001B[36mPipeline._validate_steps\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    336\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m    337\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mhasattr\u001B[39m(t, \u001B[33m\"\u001B[39m\u001B[33mfit\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(t, \u001B[33m\"\u001B[39m\u001B[33mfit_transform\u001B[39m\u001B[33m\"\u001B[39m)) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\n\u001B[32m    338\u001B[39m         t, \u001B[33m\"\u001B[39m\u001B[33mtransform\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    339\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m340\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[32m    341\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mAll intermediate steps should be \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    342\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mtransformers and implement fit and transform \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    343\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mor be the string \u001B[39m\u001B[33m'\u001B[39m\u001B[33mpassthrough\u001B[39m\u001B[33m'\u001B[39m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    344\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m (type \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m) doesn\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt\u001B[39m\u001B[33m\"\u001B[39m % (t, \u001B[38;5;28mtype\u001B[39m(t))\n\u001B[32m    345\u001B[39m         )\n\u001B[32m    347\u001B[39m \u001B[38;5;66;03m# We allow last estimator to be None as an identity transformation\u001B[39;00m\n\u001B[32m    348\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    349\u001B[39m     estimator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    350\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m estimator != \u001B[33m\"\u001B[39m\u001B[33mpassthrough\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    351\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(estimator, \u001B[33m\"\u001B[39m\u001B[33mfit\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    352\u001B[39m ):\n",
      "\u001B[31mTypeError\u001B[39m: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<__main__.PartOfSpeechFeatures object at 0x000001A499B0C1A0>' (type <class '__main__.PartOfSpeechFeatures'>) doesn't"
     ]
    }
   ],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
